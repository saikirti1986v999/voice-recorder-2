{"id":3392,"date":"2023-09-02T02:06:20","date_gmt":"2023-09-01T21:06:20","guid":{"rendered":"https:\/\/jumpshare.com\/blog\/?p=3392"},"modified":"2023-09-02T02:06:20","modified_gmt":"2023-09-01T21:06:20","slug":"deep-learning-on-aws-graviton2-nvidia-tensor-t4g-for-as-low-as-free-with-cuda-12-2","status":"publish","type":"post","link":"https:\/\/jumpshare.com\/blog\/deep-learning-on-aws-graviton2-nvidia-tensor-t4g-for-as-low-as-free-with-cuda-12-2\/","title":{"rendered":"Deep Learning on AWS Graviton2, NVIDIA Tensor T4G for as Low as Free with CUDA-12.2"},"content":{"rendered":"<section class=\"section section--body\">\n<div class=\"section-content\">\n<figure><img loading=\"lazy\" decoding=\"async\" class=\"alignnone wp-image-3396 size-full\" src=\"https:\/\/blog-cdn.jumpshare.com\/blog\/wp-content\/uploads\/2023\/09\/pexels-google-deepmind-18069365.jpg\" alt=\"\" width=\"1920\" height=\"1080\" srcset=\"https:\/\/blog-cdn.jumpshare.com\/blog\/wp-content\/uploads\/2023\/09\/pexels-google-deepmind-18069365.jpg 1920w, https:\/\/blog-cdn.jumpshare.com\/blog\/wp-content\/uploads\/2023\/09\/pexels-google-deepmind-18069365-350x197.jpg 350w, https:\/\/blog-cdn.jumpshare.com\/blog\/wp-content\/uploads\/2023\/09\/pexels-google-deepmind-18069365-1024x576.jpg 1024w, https:\/\/blog-cdn.jumpshare.com\/blog\/wp-content\/uploads\/2023\/09\/pexels-google-deepmind-18069365-768x432.jpg 768w, https:\/\/blog-cdn.jumpshare.com\/blog\/wp-content\/uploads\/2023\/09\/pexels-google-deepmind-18069365-1536x864.jpg 1536w\" sizes=\"(max-width: 1920px) 100vw, 1920px\" \/><\/figure>\n<div class=\"section-inner sectionLayout--insetColumn\">\n<figure class=\"graf graf--figure\"><\/figure>\n<p><em><strong>Note:<\/strong> The \u201cas low as free\u201d tagline is based on g5g.xlarge spot instance rates, which have been as low as $0.1519\/hr.<\/em><\/p>\n<h2 class=\"graf graf--h3\" style=\"text-align: left;\">Introduction<\/h2>\n<p class=\"graf graf--p\" style=\"text-align: left;\">The world we live in today heavily relies on artificial intelligence. From vacuum bots to sales support, from self-driving cars to disease detection, from finding the content you want to consume to translating from a foreign language to your native one. AI is behind every great product out there, and the need for an efficient, cost-effective, and scalable deep learning architecture has never been more critical.<\/p>\n<p class=\"graf graf--p\" style=\"text-align: left;\">The G5g instances powered by Amazon\u2019s own Graviton2 processor and also feature NVIDIA T4G Tensor Core GPUs are a cost-effective alternative to Intel\u2019s and AMD\u2019s powered instances for deploying deep learning applications.<\/p>\n<h2 class=\"graf graf--h3\" style=\"text-align: left;\">The Dilemma<\/h2>\n<p class=\"graf graf--p\" style=\"text-align: left;\">AWS offers robust, powerful, cost-effective architecture for running artificial intelligence and deep learning tasks. One of the advantages is the option to use spot instances, which are far more cost-effective at times and up to 70% cheaper than on-demand instances.<\/p>\n<p class=\"graf graf--p\" style=\"text-align: left;\">For example, the spot pricing history for the <code class=\"markup--code markup--p-code\">g5g.xlarge<\/code> instance in various \u201cus-east\u201d zones ranged from <strong class=\"markup--strong markup--p-strong\">$0.1720<\/strong> to <strong class=\"markup--strong markup--p-strong\">$0.1519<\/strong> per hour for the past three months. These rates are tempting, but at the time of writing, no official <strong class=\"markup--strong markup--p-strong\">Amazon Linux 2023<\/strong> Deep learning AMI is available for the Amazon <strong class=\"markup--strong markup--p-strong\">G5g<\/strong> instances family. Setting up the environment can be cumbersome: finding drivers, the correct dev toolchain, and a pre-compiled PyTorch module supporting the latest DL toolkit<\/p>\n<p class=\"graf graf--p\" style=\"text-align: left;\"><img loading=\"lazy\" decoding=\"async\" class=\"alignnone wp-image-3430 size-full\" src=\"https:\/\/blog-cdn.jumpshare.com\/blog\/wp-content\/uploads\/2023\/09\/1-yceDwTCoFaPOHWvYJKGkxw2-1.png\" alt=\"\" width=\"1600\" height=\"1001\" srcset=\"https:\/\/blog-cdn.jumpshare.com\/blog\/wp-content\/uploads\/2023\/09\/1-yceDwTCoFaPOHWvYJKGkxw2-1.png 1600w, https:\/\/blog-cdn.jumpshare.com\/blog\/wp-content\/uploads\/2023\/09\/1-yceDwTCoFaPOHWvYJKGkxw2-1-350x219.png 350w, https:\/\/blog-cdn.jumpshare.com\/blog\/wp-content\/uploads\/2023\/09\/1-yceDwTCoFaPOHWvYJKGkxw2-1-1024x641.png 1024w, https:\/\/blog-cdn.jumpshare.com\/blog\/wp-content\/uploads\/2023\/09\/1-yceDwTCoFaPOHWvYJKGkxw2-1-768x480.png 768w, https:\/\/blog-cdn.jumpshare.com\/blog\/wp-content\/uploads\/2023\/09\/1-yceDwTCoFaPOHWvYJKGkxw2-1-1536x961.png 1536w\" sizes=\"(max-width: 1600px) 100vw, 1600px\" \/><\/p>\n<p><em>Spot price history for g5g.xlarge for the last three months.<\/em><\/p>\n<h2 class=\"graf graf--h3\" style=\"text-align: left;\">Navigating the Challenge\u200a\u2014\u200aA How-To\u00a0Guide<\/h2>\n<p class=\"graf graf--p\" style=\"text-align: left;\">This aims to bridge the gap by offering comprehensive step-by-step instructions suitable for newcomers and seasoned data scientists. The goal is to enable you to leverage these state-of-the-art technologies at a meager cost without the hassle of finding the right driver and packages for the G5g family. Eventually, we will compile all the individual steps into a single script that will further streamline the process.<\/p>\n<h3 class=\"graf graf--h4\" style=\"text-align: left;\">1. Launching an\u00a0Instance<\/h3>\n<p class=\"graf graf--p\" style=\"text-align: left;\">For setting up an instance, we\u2019ll use <code class=\"markup--code markup--p-code\">g5g.4xlarge<\/code> instance. The idea behind using a more powerful instance is to accelerate compilation time. We will launch the build instance with the AWS Command Line Interface (aws cli).<\/p>\n<p class=\"graf graf--p\" style=\"text-align: left;\">First, set the following environment variables:<\/p>\n<ul class=\"postList\" style=\"text-align: left;\">\n<li class=\"graf graf--li\"><code class=\"markup--code markup--li-code\">REGION<\/code>: Specifies the AWS region, e.g., \u2018us-east-1\u2019.<\/li>\n<li class=\"graf graf--li\"><code class=\"markup--code markup--li-code\">SECURITY_GROUPS<\/code>: Your security group ID(s).<\/li>\n<li class=\"graf graf--li\"><code class=\"markup--code markup--li-code\">KEY_PAIR<\/code>: The name of your SSH key pair.<\/li>\n<li class=\"graf graf--li\"><code class=\"markup--code markup--li-code\">SUBNET<\/code>: The ID of your subnet.<\/li>\n<\/ul>\n<p class=\"graf graf--p\" style=\"text-align: left;\">If you have any confusion about these variables. You can refer to the <a class=\"markup--anchor markup--p-anchor\" href=\"https:\/\/docs.aws.amazon.com\/vpc\/latest\/userguide\/security-groups.html\" target=\"_blank\" rel=\"noopener\" data-href=\"https:\/\/docs.aws.amazon.com\/vpc\/latest\/userguide\/security-groups.html\">security group,<\/a> <a class=\"markup--anchor markup--p-anchor\" href=\"https:\/\/docs.aws.amazon.com\/AWSEC2\/latest\/UserGuide\/ec2-key-pairs.html\" target=\"_blank\" rel=\"noopener\" data-href=\"https:\/\/docs.aws.amazon.com\/AWSEC2\/latest\/UserGuide\/ec2-key-pairs.html\">keypair,<\/a> and <a class=\"markup--anchor markup--p-anchor\" href=\"https:\/\/docs.aws.amazon.com\/vpc\/latest\/userguide\/configure-subnets.html\" target=\"_blank\" rel=\"noopener\" data-href=\"https:\/\/docs.aws.amazon.com\/vpc\/latest\/userguide\/configure-subnets.html\">subnets<\/a> documentation.<\/p>\n<p class=\"graf graf--p\" style=\"text-align: left;\">Once you have these values, you can set these variables like this.<\/p>\n<pre class=\"graf graf--pre graf--preV2\" spellcheck=\"false\" data-code-block-mode=\"2\" data-code-block-lang=\"bash\"><span class=\"pre--content\"><span class=\"hljs-built_in\">export<\/span> REGION=<span class=\"hljs-string\">'us-east-1'<\/span>\r\n<span class=\"hljs-built_in\">export<\/span> SECURITY_GROUPS=<span class=\"hljs-string\">'YourFirstSecurityGroupIdsHere'<\/span>\r\n<span class=\"hljs-built_in\">export<\/span> KEY_PAIR=<span class=\"hljs-string\">'YourSSHKeyNameHere'<\/span>\r\n<span class=\"hljs-built_in\">export<\/span> SUBNET=<span class=\"hljs-string\">'YourSubnetHere'<\/span><\/span><\/pre>\n<p class=\"graf graf--p\" style=\"text-align: left;\">Next, we need to find the latest <strong class=\"markup--strong markup--p-strong\">Amazon Linux 2023 AMI ID<\/strong> so you will get the latest AMI every time you run this script. The following command will fetch the AMI ID and store it as <code class=\"markup--code markup--p-code\">AMI_ID<\/code>.<\/p>\n<p class=\"graf graf--p\" style=\"text-align: left;\">Let\u2019s launch the instance using the AMI ID we retrieved earlier by executing:<\/p>\n<pre class=\"graf graf--pre graf--preV2\" spellcheck=\"false\" data-code-block-mode=\"1\" data-code-block-lang=\"wasm\"><span class=\"pre--content\">aws ec2 run-instances \\\r\n  --image-id <span class=\"hljs-variable\">$AMI_ID<\/span> \\\r\n  --instance-<span class=\"hljs-keyword\">type<\/span> g5g.4xlarge \\\r\n  --key-name <span class=\"hljs-variable\">$KEY_PAIR<\/span> \\\r\n  --subnet-id <span class=\"hljs-variable\">$SUBNET<\/span> \\\r\n  --security-group-ids <span class=\"hljs-variable\">$SECURITY_GROUPS<\/span> \\\r\n  --region <span class=\"hljs-variable\">$REGION<\/span> \\\r\n  --<span class=\"hljs-keyword\">block<\/span>-device-mappings '[{<span class=\"hljs-string\">\"DeviceName\"<\/span>:<span class=\"hljs-string\">\"\/dev\/xvda\"<\/span>,<span class=\"hljs-string\">\"Ebs\"<\/span>:{<span class=\"hljs-string\">\"VolumeSize\"<\/span>:<span class=\"hljs-number\">20<\/span>,<span class=\"hljs-string\">\"VolumeType\"<\/span>:<span class=\"hljs-string\">\"gp3\"<\/span>}}]' \\\r\n  --tag-specifications 'ResourceType=instance,Tags=[{Key=Name,Value=AMI-Builder}]'<\/span><\/pre>\n<p class=\"graf graf--p\" style=\"text-align: left;\">This command initiates a <code class=\"markup--code markup--p-code\">g5g.4xlarge<\/code> instance with the Latest Amazon Linux 2023 AMI ID. It also configures the instance to use the specified security groups, key pair, and subnet we provided in environment variables. We\u2019ve also attached 20 GB of storage to the root device for downloading different libraries and PyTorch compilation.<\/p>\n<h3 class=\"graf graf--h4\" style=\"text-align: left;\">2. Installing System Updates and Required\u00a0Packages<\/h3>\n<p class=\"graf graf--p\" style=\"text-align: left;\">Setting up any machine, be it local or in the cloud, it is always a good practice to keep it updated. This part will install all the updates and tools used in compilation or running AI tasks.<br \/>\nBut before going to Gung Ho, We recommend taking an overview of the guide first and checking the complete script at the end of this tutorial, which should save you from lots of trouble.<\/p>\n<p class=\"graf graf--p\" style=\"text-align: left;\">First, let\u2019s define some essential environment variables.<\/p>\n<pre class=\"graf graf--pre graf--preV2\" spellcheck=\"false\" data-code-block-mode=\"2\" data-code-block-lang=\"bash\"><span class=\"pre--content\">CUDA_HOME=\/usr\/local\/cuda\r\nHOME_DIR=\/home\/ec2-user<\/span><\/pre>\n<p class=\"graf graf--p\" style=\"text-align: left;\">Now, we\u2019ll create a function called <code class=\"markup--code markup--p-code\">install_utils<\/code> that carries out a series of tasks.<\/p>\n<pre class=\"graf graf--pre graf--preV2\" spellcheck=\"false\" data-code-block-mode=\"2\" data-code-block-lang=\"bash\"><span class=\"pre--content\"><span class=\"hljs-function\"><span class=\"hljs-title\">install_utils<\/span><\/span>() {\r\n    <span class=\"hljs-comment\"># Update all system packages to their latest versions<\/span>\r\n    dnf -y update\r\n\r\n    <span class=\"hljs-comment\"># Install development tools, which include compilers and other utilities<\/span>\r\n    dnf -y groupinstall <span class=\"hljs-string\">\"Development Tools\"<\/span>\r\n\r\n    <span class=\"hljs-comment\"># Install the packages that are specifically required for our setup<\/span>\r\n    dnf install -y openssl-devel cmake3 rust cargo\r\n    dnf install -y amazon-efs-utils htop iotop yasm nasm jq python3-pip python-devel cronie cronie-anacron\r\n\r\n    <span class=\"hljs-comment\"># Add necessary paths to the .bashrc file<\/span>\r\n    <span class=\"hljs-built_in\">echo<\/span> <span class=\"hljs-string\">\"PATH=<span class=\"hljs-variable\">$CUDA_HOME<\/span>\/bin:\\$PATH\"<\/span> | sudo <span class=\"hljs-built_in\">tee<\/span> -a <span class=\"hljs-variable\">$HOME_DIR<\/span>\/.bashrc\r\n    <span class=\"hljs-built_in\">echo<\/span> <span class=\"hljs-string\">\"LD_LIBRARY_PATH=<span class=\"hljs-variable\">$CUDA_HOME<\/span>\/lib64:\\$LD_LIBRARY_PATH\"<\/span> | sudo <span class=\"hljs-built_in\">tee<\/span> -a <span class=\"hljs-variable\">$HOME_DIR<\/span>\/.bashrc\r\n\r\n    <span class=\"hljs-comment\"># Configure shared libraries<\/span>\r\n    <span class=\"hljs-built_in\">echo<\/span> <span class=\"hljs-string\">\"\/usr\/local\/lib\"<\/span> | sudo <span class=\"hljs-built_in\">tee<\/span> \/etc\/ld.so.conf.d\/usr-local-lib.conf\r\n    <span class=\"hljs-built_in\">echo<\/span> <span class=\"hljs-string\">\"\/usr\/local\/lib64\"<\/span> | sudo <span class=\"hljs-built_in\">tee<\/span> -a \/etc\/ld.so.conf.d\/usr-local-lib.conf\r\n}<\/span><\/pre>\n<p class=\"graf graf--p\" style=\"text-align: left;\">By running this <code class=\"markup--code markup--p-code\">install_utils<\/code> function, you will have an updated OS and development tools needed in later steps.<\/p>\n<h3 class=\"graf graf--h4\" style=\"text-align: left;\">3. Installing Latest NVIDIA Drivers, CUDA 12.2 Toolkit, and Cuda Deep Neural Network Library<\/h3>\n<p class=\"graf graf--p\" style=\"text-align: left;\">In this step, we will install the NVIDIA GPU driver, the latest CUDA 12.2 toolkit, and CUDA Deep Neural Network (CuDNN) libraries. This part uses the latest driver and toolkit released on August 29, 2023. If you read it later, you can update the URLs for the latest driver and libraries; everything else will be the same. Steps to find the latest driver, toolkit, and library are also mentioned below.<\/p>\n<h4 class=\"graf graf--h4\" style=\"text-align: left;\"><strong class=\"markup--strong markup--h4-strong\">Install NVIDIA GPU\u00a0Driver<\/strong><\/h4>\n<p class=\"graf graf--p\" style=\"text-align: left;\">To download and install the NVIDIA Tesla T4G driver, execute<\/p>\n<pre class=\"graf graf--pre graf--preV2\" spellcheck=\"false\" data-code-block-mode=\"2\" data-code-block-lang=\"bash\"><span class=\"pre--content\">wget https:\/\/us.download.nvidia.com\/tesla\/535.104.05\/NVIDIA-Linux-aarch64-535.104.05.run\r\nsh NVIDIA-Linux-aarch64-535.104.05.run --disable-nouveau --silent<\/span><\/pre>\n<p class=\"graf graf--p\" style=\"text-align: left;\">If everything goes smoothly; you should have a working NVIDIA driver by now, which can be checked by running the NVIDIA system management interface command <code class=\"markup--code markup--p-code\">nvidia-smi<\/code> in the terminal.<\/p>\n<p><img loading=\"lazy\" decoding=\"async\" class=\"alignnone wp-image-3434 size-full\" src=\"https:\/\/blog-cdn.jumpshare.com\/blog\/wp-content\/uploads\/2023\/09\/nvidia-smi.png\" alt=\"\" width=\"1600\" height=\"706\" srcset=\"https:\/\/blog-cdn.jumpshare.com\/blog\/wp-content\/uploads\/2023\/09\/nvidia-smi.png 1600w, https:\/\/blog-cdn.jumpshare.com\/blog\/wp-content\/uploads\/2023\/09\/nvidia-smi-350x154.png 350w, https:\/\/blog-cdn.jumpshare.com\/blog\/wp-content\/uploads\/2023\/09\/nvidia-smi-1024x452.png 1024w, https:\/\/blog-cdn.jumpshare.com\/blog\/wp-content\/uploads\/2023\/09\/nvidia-smi-768x339.png 768w, https:\/\/blog-cdn.jumpshare.com\/blog\/wp-content\/uploads\/2023\/09\/nvidia-smi-1536x678.png 1536w\" sizes=\"(max-width: 1600px) 100vw, 1600px\" \/><\/p>\n<figure class=\"graf graf--figure\"><\/figure>\n<\/div>\n<\/div>\n<\/section>\n<section class=\"section section--body\">\n<div class=\"section-divider\">\n<p><em>nvidia-smi\u200a\u2014\u200aNVIDIA System Management Interface<\/em><\/p>\n<hr class=\"section-divider\" \/>\n<\/div>\n<div class=\"section-content\">\n<div class=\"section-inner sectionLayout--insetColumn\">\n<p class=\"graf graf--p\">The latest drivers for NVIDIA Tesla T4G can be found <a class=\"markup--anchor markup--p-anchor\" href=\"https:\/\/www.nvidia.com\/Download\/Find.aspx\" target=\"_blank\" rel=\"noopener\" data-href=\"https:\/\/www.nvidia.com\/Download\/Find.aspx\">here<\/a> by selecting the following options.<\/p>\n<p><img loading=\"lazy\" decoding=\"async\" class=\"alignnone wp-image-3435 size-full\" src=\"https:\/\/blog-cdn.jumpshare.com\/blog\/wp-content\/uploads\/2023\/09\/driver-download.png\" alt=\"\" width=\"1384\" height=\"926\" srcset=\"https:\/\/blog-cdn.jumpshare.com\/blog\/wp-content\/uploads\/2023\/09\/driver-download.png 1384w, https:\/\/blog-cdn.jumpshare.com\/blog\/wp-content\/uploads\/2023\/09\/driver-download-350x234.png 350w, https:\/\/blog-cdn.jumpshare.com\/blog\/wp-content\/uploads\/2023\/09\/driver-download-1024x685.png 1024w, https:\/\/blog-cdn.jumpshare.com\/blog\/wp-content\/uploads\/2023\/09\/driver-download-768x514.png 768w\" sizes=\"(max-width: 1384px) 100vw, 1384px\" \/><\/p>\n<figure class=\"graf graf--figure\"><figcaption class=\"imageCaption\">For guidance on selecting the correct driver, refer to the options\u00a0above.\u00a0<\/figcaption><\/figure>\n<\/div>\n<\/div>\n<\/section>\n<section class=\"section section--body\">\n<div class=\"section-divider\">\n<hr class=\"section-divider\" \/>\n<\/div>\n<div class=\"section-content\">\n<div class=\"section-inner sectionLayout--insetColumn\">\n<h4 class=\"graf graf--h4\">Install CUDA\u00a0Toolkit<\/h4>\n<p class=\"graf graf--p\">The next step involves downloading and installing the CUDA 12.2 toolkit. which can be done by running following bash commands<\/p>\n<pre class=\"graf graf--pre graf--preV2\" spellcheck=\"false\" data-code-block-mode=\"2\" data-code-block-lang=\"bash\"><span class=\"pre--content\">wget https:\/\/developer.download.nvidia.com\/compute\/cuda\/12.2.2\/local_installers\/cuda_12.2.2_535.104.05_linux_sbsa.run\r\nsh cuda_12.2.2_535.104.05_linux_sbsa.run --silent --override \\\r\n    --toolkit --samples --toolkitpath=\/usr\/local\/cuda-12.2 \\\r\n    --samplespath=<span class=\"hljs-variable\">$CUDA_HOME<\/span> --no-opengl-libs \r\n\r\n<\/span><\/pre>\n<p class=\"graf graf--p\">To find the latest version, visit <a class=\"markup--anchor markup--p-anchor\" href=\"https:\/\/developer.nvidia.com\/cuda-toolkit\" target=\"_blank\" rel=\"noopener\" data-href=\"https:\/\/developer.nvidia.com\/cuda-toolkit\">NVIDIA\u2019s developer page<\/a> and use the following selection.<\/p>\n<p><img loading=\"lazy\" decoding=\"async\" class=\"alignnone wp-image-3436 size-full\" src=\"https:\/\/blog-cdn.jumpshare.com\/blog\/wp-content\/uploads\/2023\/09\/cuda-toolkit.png\" alt=\"\" width=\"1600\" height=\"936\" srcset=\"https:\/\/blog-cdn.jumpshare.com\/blog\/wp-content\/uploads\/2023\/09\/cuda-toolkit.png 1600w, https:\/\/blog-cdn.jumpshare.com\/blog\/wp-content\/uploads\/2023\/09\/cuda-toolkit-350x205.png 350w, https:\/\/blog-cdn.jumpshare.com\/blog\/wp-content\/uploads\/2023\/09\/cuda-toolkit-1024x599.png 1024w, https:\/\/blog-cdn.jumpshare.com\/blog\/wp-content\/uploads\/2023\/09\/cuda-toolkit-768x449.png 768w, https:\/\/blog-cdn.jumpshare.com\/blog\/wp-content\/uploads\/2023\/09\/cuda-toolkit-1536x899.png 1536w\" sizes=\"(max-width: 1600px) 100vw, 1600px\" \/><\/p>\n<figure class=\"graf graf--figure\"><\/figure>\n<\/div>\n<\/div>\n<\/section>\n<section class=\"section section--body\">\n<div class=\"section-divider\">\n<hr class=\"section-divider\" \/>\n<\/div>\n<div class=\"section-content\">\n<div class=\"section-inner sectionLayout--insetColumn\">\n<h4 class=\"graf graf--h4\">Install NVIDIA CUDA\u00ae Deep Neural Network Library (cuDNN)<\/h4>\n<p class=\"graf graf--p\">Lastly, we\u2019ll install the CuDNN library for \u201cServer Base System Architecture (SBSA)\u201d.<\/p>\n<pre class=\"graf graf--pre graf--preV2\" spellcheck=\"false\" data-code-block-mode=\"2\" data-code-block-lang=\"bash\"><span class=\"pre--content\">wget https:\/\/developer.download.nvidia.com\/compute\/cudnn\/redist\/cudnn\/linux-sbsa\/cudnn-linux-sbsa-8.9.4.25_cuda12-archive.tar.xz\r\n    tar -xf cudnn-linux-sbsa-8.9.4.25_cuda12-archive.tar.xz\r\n    <span class=\"hljs-built_in\">cp<\/span> -P cudnn-linux-sbsa-8.9.4.25_cuda12-archive\/include\/* <span class=\"hljs-variable\">$CUDA_HOME<\/span>\/include\/\r\n    <span class=\"hljs-built_in\">cp<\/span> -P cudnn-linux-sbsa-8.9.4.25_cuda12-archive\/lib\/* <span class=\"hljs-variable\">$CUDA_HOME<\/span>\/lib64\/\r\n    <span class=\"hljs-built_in\">chmod<\/span> a+r <span class=\"hljs-variable\">$CUDA_HOME<\/span>\/lib64\/*<\/span><\/pre>\n<p class=\"graf graf--p\">Latest cuDNN can be downloaded from <a class=\"markup--anchor markup--p-anchor\" href=\"https:\/\/developer.download.nvidia.com\/compute\/cudnn\/redist\/cudnn\/linux-sbsa\/\" target=\"_blank\" rel=\"noopener\" data-href=\"https:\/\/developer.download.nvidia.com\/compute\/cudnn\/redist\/cudnn\/linux-sbsa\/\">here<\/a>.<\/p>\n<p><img loading=\"lazy\" decoding=\"async\" class=\"alignnone wp-image-3437 size-full\" src=\"https:\/\/blog-cdn.jumpshare.com\/blog\/wp-content\/uploads\/2023\/09\/cudnn-libraries.png\" alt=\"\" width=\"1600\" height=\"828\" srcset=\"https:\/\/blog-cdn.jumpshare.com\/blog\/wp-content\/uploads\/2023\/09\/cudnn-libraries.png 1600w, https:\/\/blog-cdn.jumpshare.com\/blog\/wp-content\/uploads\/2023\/09\/cudnn-libraries-350x181.png 350w, https:\/\/blog-cdn.jumpshare.com\/blog\/wp-content\/uploads\/2023\/09\/cudnn-libraries-1024x530.png 1024w, https:\/\/blog-cdn.jumpshare.com\/blog\/wp-content\/uploads\/2023\/09\/cudnn-libraries-768x397.png 768w, https:\/\/blog-cdn.jumpshare.com\/blog\/wp-content\/uploads\/2023\/09\/cudnn-libraries-1536x795.png 1536w\" sizes=\"(max-width: 1600px) 100vw, 1600px\" \/><\/p>\n<figure class=\"graf graf--figure\"><figcaption class=\"imageCaption\">List of available cuDNN sbsa libraries for CUDA 11 and CUDA\u00a012.\u00a0<\/figcaption><\/figure>\n<p class=\"graf graf--p\">By combining all three, we will have the following function, which we will use in the final script as well.<\/p>\n<pre class=\"graf graf--pre graf--preV2\" spellcheck=\"false\" data-code-block-mode=\"2\" data-code-block-lang=\"bash\"><span class=\"pre--content\"><span class=\"hljs-function\"><span class=\"hljs-title\">setup_gpu<\/span><\/span>() {\r\n    wget https:\/\/us.download.nvidia.com\/tesla\/535.104.05\/NVIDIA-Linux-aarch64-535.104.05.run\r\n    sh NVIDIA-Linux-aarch64-535.104.05.run --disable-nouveau --silent\r\n\r\n    wget https:\/\/developer.download.nvidia.com\/compute\/cuda\/12.2.2\/local_installers\/cuda_12.2.2_535.104.05_linux_sbsa.run\r\n    sh cuda_12.2.2_535.104.05_linux_sbsa.run --silent --override --toolkit --samples --toolkitpath=\/usr\/local\/cuda-12.2 --samplespath=<span class=\"hljs-variable\">$CUDA_HOME<\/span> --no-opengl-libs\r\n\r\n    wget https:\/\/developer.download.nvidia.com\/compute\/cudnn\/redist\/cudnn\/linux-sbsa\/cudnn-linux-sbsa-8.9.4.25_cuda12-archive.tar.xz\r\n    tar -xf cudnn-linux-sbsa-8.9.4.25_cuda12-archive.tar.xz\r\n    <span class=\"hljs-built_in\">cp<\/span> -P cudnn-linux-sbsa-8.9.4.25_cuda12-archive\/include\/* <span class=\"hljs-variable\">$CUDA_HOME<\/span>\/include\/\r\n    <span class=\"hljs-built_in\">cp<\/span> -P cudnn-linux-sbsa-8.9.4.25_cuda12-archive\/lib\/* <span class=\"hljs-variable\">$CUDA_HOME<\/span>\/lib64\/\r\n    <span class=\"hljs-built_in\">chmod<\/span> a+r <span class=\"hljs-variable\">$CUDA_HOME<\/span>\/lib64\/*\r\n    ldconfig\r\n}<\/span><\/pre>\n<\/div>\n<\/div>\n<\/section>\n<section class=\"section section--body\">\n<div class=\"section-divider\">\n<hr class=\"section-divider\" \/>\n<\/div>\n<div class=\"section-content\">\n<div class=\"section-inner sectionLayout--insetColumn\">\n<h3 class=\"graf graf--h4\">4. Compiling and Installing CUDA 12.2 Enabled PyTorch<\/h3>\n<p class=\"graf graf--p\">Next, we will compile and install PyTotch from source with the latest CUDA support for ARM-based ec2 instances, along with all the necessary Python packages.<\/p>\n<pre class=\"graf graf--pre graf--preV2\" spellcheck=\"false\" data-code-block-mode=\"2\" data-code-block-lang=\"bash\"><span class=\"pre--content\"><span class=\"hljs-comment\"># Download and install ccache for faster compilation<\/span>\r\nwget https:\/\/github.com\/ccache\/ccache\/releases\/download\/v4.8.3\/ccache-4.8.3.tar.xz\r\ntar -xf ccache-4.8.3.tar.xz\r\n<span class=\"hljs-built_in\">pushd<\/span> ccache-4.8.3\r\ncmake .\r\nmake -j <span class=\"hljs-variable\">$CPUS<\/span>\r\nmake install\r\n<span class=\"hljs-built_in\">popd<\/span>\r\n\r\n<span class=\"hljs-comment\"># Install NumPy, a dependency for PyTorch<\/span>\r\ndnf install -y numpy\r\n\r\n<span class=\"hljs-comment\"># Install Python typing extensions for better type-checking<\/span>\r\nsudo -u ec2-user pip3 install typing-extensions\r\n\r\n<span class=\"hljs-comment\"># Clone PyTorch repository and install from source<\/span>\r\ngit <span class=\"hljs-built_in\">clone<\/span> --recursive https:\/\/github.com\/pytorch\/pytorch.git\r\n<span class=\"hljs-built_in\">pushd<\/span> pytorch\r\npython3 setup.py install\r\n<span class=\"hljs-built_in\">popd<\/span>\r\n\r\n<span class=\"hljs-comment\"># Refresh the dynamic linker run-time bindings<\/span>\r\nldconfig\r\n\r\n<span class=\"hljs-comment\"># Install additional Python libraries for PyTorch<\/span>\r\nsudo -u ec2-user pip3 install sympy filelock fsspec networkx\r\n<\/span><\/pre>\n<h3 class=\"graf graf--h4\">5. Test Your Installation<\/h3>\n<p class=\"graf graf--p\">After you\u2019ve gone through the installation process, you\u2019ll want to ensure that PyTorch and CUDA are working as expected. Run the following command to test the setup.<\/p>\n<pre class=\"graf graf--pre graf--preV2\" spellcheck=\"false\" data-code-block-mode=\"2\" data-code-block-lang=\"bash\"><span class=\"pre--content\">python3 -c <span class=\"hljs-string\">\"import torch; print('Using device: ', torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\"<\/span>;<\/span><\/pre>\n<p class=\"graf graf--p\">If the device returns \u2018cuda,\u2019 then congratulations, you\u2019ve successfully installed PyTorch with latest CUDA support!<\/p>\n<h2 class=\"graf graf--h3\">Complete Script for Effortless Setup \ud83e\ude84<\/h2>\n<p><img loading=\"lazy\" decoding=\"async\" class=\"alignnone wp-image-3438 size-full\" src=\"https:\/\/blog-cdn.jumpshare.com\/blog\/wp-content\/uploads\/2023\/09\/mix-colors.png\" alt=\"\" width=\"1600\" height=\"813\" srcset=\"https:\/\/blog-cdn.jumpshare.com\/blog\/wp-content\/uploads\/2023\/09\/mix-colors.png 1600w, https:\/\/blog-cdn.jumpshare.com\/blog\/wp-content\/uploads\/2023\/09\/mix-colors-350x178.png 350w, https:\/\/blog-cdn.jumpshare.com\/blog\/wp-content\/uploads\/2023\/09\/mix-colors-1024x520.png 1024w, https:\/\/blog-cdn.jumpshare.com\/blog\/wp-content\/uploads\/2023\/09\/mix-colors-768x390.png 768w, https:\/\/blog-cdn.jumpshare.com\/blog\/wp-content\/uploads\/2023\/09\/mix-colors-1536x780.png 1536w\" sizes=\"(max-width: 1600px) 100vw, 1600px\" \/><\/p>\n<figure class=\"graf graf--figure\"><\/figure>\n<p class=\"graf graf--p\">Ready for some magic? Before getting started, ensure that your AWS CLI is properly configured. If you haven\u2019t done this, refer to the <a class=\"markup--anchor markup--p-anchor\" href=\"https:\/\/docs.aws.amazon.com\/cli\/latest\/userguide\/cli-chap-welcome.html\" target=\"_blank\" rel=\"noopener\" data-href=\"https:\/\/docs.aws.amazon.com\/cli\/latest\/userguide\/cli-chap-welcome.html\">AWS documentation<\/a> to get up to speed. You will also need to gather the IDs for your <a class=\"markup--anchor markup--p-anchor\" href=\"https:\/\/docs.aws.amazon.com\/vpc\/latest\/userguide\/security-groups.html\" target=\"_blank\" rel=\"noopener\" data-href=\"https:\/\/docs.aws.amazon.com\/vpc\/latest\/userguide\/security-groups.html\">security group<\/a> and <a class=\"markup--anchor markup--p-anchor\" href=\"https:\/\/docs.aws.amazon.com\/vpc\/latest\/userguide\/configure-subnets.html\" target=\"_blank\" rel=\"noopener\" data-href=\"https:\/\/docs.aws.amazon.com\/vpc\/latest\/userguide\/configure-subnets.html\">subnet<\/a> and the name of your <a class=\"markup--anchor markup--p-anchor\" href=\"https:\/\/docs.aws.amazon.com\/AWSEC2\/latest\/UserGuide\/ec2-key-pairs.html\" target=\"_blank\" rel=\"noopener\" data-href=\"https:\/\/docs.aws.amazon.com\/AWSEC2\/latest\/UserGuide\/ec2-key-pairs.html\">key pair<\/a>.<\/p>\n<p class=\"graf graf--p\">Once you have completed the necessary preparations, run the provided script. This will launch a g5g.4xlarge instance pre-loaded with user data, which initiates the installation process upon launch. The entire setup process should take approximately an hour to complete. However, you can monitor the progress as it goes. To begin, SSH into your newly launched instance.<\/p>\n<pre class=\"graf graf--pre graf--preV2\" spellcheck=\"false\" data-code-block-mode=\"2\" data-code-block-lang=\"bash\"><span class=\"pre--content\">ssh -i <span class=\"hljs-string\">\"your-key-pair.pem\"<\/span> ec2-user@your-instance-ip <\/span><\/pre>\n<p class=\"graf graf--p\">Then, run the following command to monitor the installation in real-time:<\/p>\n<pre class=\"graf graf--pre graf--preV2\" spellcheck=\"false\" data-code-block-mode=\"2\" data-code-block-lang=\"bash\"><span class=\"pre--content\"><span class=\"hljs-built_in\">tail<\/span> -f \/home\/ec2-user\/install.log<\/span><\/pre>\n<p class=\"graf graf--p\">The complete script can be downloaded from <a class=\"markup--anchor markup--p-anchor\" href=\"https:\/\/gist.github.com\/bilalmughal\/0500f27454a508bd3552fcf03e3adadb\" target=\"_blank\" rel=\"noopener\" data-href=\"https:\/\/gist.github.com\/bilalmughal\/0500f27454a508bd3552fcf03e3adadb\">GitHub<\/a> and goes as follows.<\/p>\n<figure class=\"graf graf--figure graf--iframe\">\n<div class=\"aspectRatioPlaceholder is-locked\">\n<div class=\"aspectRatioPlaceholder-fill\"><\/div>\n<div class=\"iframeContainer\"><script src=\"https:\/\/gist.github.com\/bilalmughal\/0500f27454a508bd3552fcf03e3adadb.js\"><\/script><\/div>\n<\/div>\n<\/figure>\n<p class=\"graf graf--p\">After everything is done you should get the following greetings.<\/p>\n<p>&nbsp;<\/p>\n<figure class=\"graf graf--figure\"><img loading=\"lazy\" decoding=\"async\" class=\"alignnone wp-image-3439 size-full\" src=\"https:\/\/blog-cdn.jumpshare.com\/blog\/wp-content\/uploads\/2023\/09\/success.png\" alt=\"\" width=\"1600\" height=\"560\" srcset=\"https:\/\/blog-cdn.jumpshare.com\/blog\/wp-content\/uploads\/2023\/09\/success.png 1600w, https:\/\/blog-cdn.jumpshare.com\/blog\/wp-content\/uploads\/2023\/09\/success-350x123.png 350w, https:\/\/blog-cdn.jumpshare.com\/blog\/wp-content\/uploads\/2023\/09\/success-1024x358.png 1024w, https:\/\/blog-cdn.jumpshare.com\/blog\/wp-content\/uploads\/2023\/09\/success-768x269.png 768w, https:\/\/blog-cdn.jumpshare.com\/blog\/wp-content\/uploads\/2023\/09\/success-1536x538.png 1536w\" sizes=\"(max-width: 1600px) 100vw, 1600px\" \/><\/figure>\n<h2 class=\"graf graf--h3\">Using AWS Management Console<\/h2>\n<p class=\"graf graf--p\">You can also use the AWS Management console for this process as well. All you need to do is \u201cLaunch an instance\u201d from the ec2 console and then select the right AMI, Architecture, and instance type, along with other networking and security configurations you will do for launching any other instance. Don\u2019t forget to increase the volume size to 20 GB as well.<\/p>\n<figure class=\"graf graf--figure\"><img loading=\"lazy\" decoding=\"async\" class=\"alignnone wp-image-3440 size-full\" src=\"https:\/\/blog-cdn.jumpshare.com\/blog\/wp-content\/uploads\/2023\/09\/launch-instance-1.png\" alt=\"\" width=\"826\" height=\"1054\" srcset=\"https:\/\/blog-cdn.jumpshare.com\/blog\/wp-content\/uploads\/2023\/09\/launch-instance-1.png 826w, https:\/\/blog-cdn.jumpshare.com\/blog\/wp-content\/uploads\/2023\/09\/launch-instance-1-274x350.png 274w, https:\/\/blog-cdn.jumpshare.com\/blog\/wp-content\/uploads\/2023\/09\/launch-instance-1-802x1024.png 802w, https:\/\/blog-cdn.jumpshare.com\/blog\/wp-content\/uploads\/2023\/09\/launch-instance-1-768x980.png 768w\" sizes=\"(max-width: 826px) 100vw, 826px\" \/><\/figure>\n<figure class=\"graf graf--figure\"><\/figure>\n<p class=\"graf graf--p\">After selecting the right AMI, architecture, instance type, storage, and other options, configure your instance\u2019s User Data by adding custom setup commands that will run during launch.<br \/>\nTo add User Data, go to the \u2018Advanced Details\u2019 section during the \u2018Configure Instance\u2019 stage, input the desired text or file, and paste the script from the GitHub repository between the \u2018EOF\u2019 markers into the User Data text area.<\/p>\n<p>&nbsp;<\/p>\n<p><img loading=\"lazy\" decoding=\"async\" class=\"alignnone wp-image-3415 size-full\" style=\"width: 100%;\" src=\"https:\/\/blog-cdn.jumpshare.com\/blog\/wp-content\/uploads\/2023\/09\/Screenshot-2023-09-01-at-1.39.39-AM.png\" alt=\"\" width=\"984\" height=\"852\" srcset=\"https:\/\/blog-cdn.jumpshare.com\/blog\/wp-content\/uploads\/2023\/09\/Screenshot-2023-09-01-at-1.39.39-AM.png 984w, https:\/\/blog-cdn.jumpshare.com\/blog\/wp-content\/uploads\/2023\/09\/Screenshot-2023-09-01-at-1.39.39-AM-350x303.png 350w, https:\/\/blog-cdn.jumpshare.com\/blog\/wp-content\/uploads\/2023\/09\/Screenshot-2023-09-01-at-1.39.39-AM-768x665.png 768w\" sizes=\"(max-width: 984px) 100vw, 984px\" \/><\/p>\n<figure class=\"graf graf--figure\"><\/figure>\n<p class=\"graf graf--p\">Remember, this User Data script is what automates your deep learning setup, so don\u2019t skip this step!<\/p>\n<h2 class=\"graf graf--h3\">Wrapping Up<\/h2>\n<p class=\"graf graf--p\">And there you have it! A one-stop solution to make your deep learning setup on an Amazon EC2 Graviton2 ARM-based instance much easier. After following these steps, you can create an AMI (Amazon Machine Image) and use it for deep-learning tasks. You should also try out spot instances for your interruptible artificial intelligence inferences, as it could save you a lot on operational costs!<br \/>\nWith this guide, we made configuration and setup hassle-free so you can dive straight into the work that matters most to you. If you find this script as helpful as we do, we would love to hear about the exciting projects it\u2019s helping you accomplish. Feel free to share your success stories and any ingenious modifications you\u2019ve made. Happy coding!<\/p>\n<h3 class=\"graf graf--h4\">\ud83d\udca1 Pro Tip: Max Power, Min Price\u200a\u2014\u200aThe G5G Magic Equation!<\/h3>\n<p class=\"graf graf--p\">Did you know the <code class=\"markup--code markup--p-code\">g5g.xlarge<\/code>,<code class=\"markup--code markup--p-code\"> g5g.2xlarge<\/code>, <code class=\"markup--code markup--p-code\">g5g.4xlarge<\/code>and <code class=\"markup--code markup--p-code\">g5g.8xlarge<\/code> <a href=\"https:\/\/instances.vantage.sh\/?selected=g5g..x%7Cg5g.x\" target=\"_blank\" rel=\"noopener\">have the same GPU power<\/a>? If increasing the CPU power or adding more memory doesn\u2019t significantly improve the performance of your application, you can stick with the <code class=\"markup--code markup--p-code\">g5g.xlarge<\/code> to save some money!<\/p>\n<p><img loading=\"lazy\" decoding=\"async\" class=\"alignnone size-full wp-image-3441\" src=\"https:\/\/blog-cdn.jumpshare.com\/blog\/wp-content\/uploads\/2023\/09\/g5g-specs.png\" alt=\"\" width=\"1600\" height=\"432\" srcset=\"https:\/\/blog-cdn.jumpshare.com\/blog\/wp-content\/uploads\/2023\/09\/g5g-specs.png 1600w, https:\/\/blog-cdn.jumpshare.com\/blog\/wp-content\/uploads\/2023\/09\/g5g-specs-350x95.png 350w, https:\/\/blog-cdn.jumpshare.com\/blog\/wp-content\/uploads\/2023\/09\/g5g-specs-1024x276.png 1024w, https:\/\/blog-cdn.jumpshare.com\/blog\/wp-content\/uploads\/2023\/09\/g5g-specs-768x207.png 768w, https:\/\/blog-cdn.jumpshare.com\/blog\/wp-content\/uploads\/2023\/09\/g5g-specs-1536x415.png 1536w\" sizes=\"(max-width: 1600px) 100vw, 1600px\" \/><\/p>\n<figure class=\"graf graf--figure\"><\/figure>\n<\/div>\n<\/div>\n<\/section>\n<section class=\"section section--body\">\n<div class=\"section-content\">\n<div class=\"section-inner sectionLayout--insetColumn\">\n<p><em>G5g Instance specification details.<\/em><\/p>\n<h2 class=\"graf graf--h3\">About the Author and Our Journey at Jumpshare<\/h2>\n<p class=\"graf graf--p\">I have been a part of the tech industry for 18 years, serving different roles and devising different engineering solutions throughout. The ever-changing landscape of the tech world and the challenges it brings excite me, especially in the area of cloud computing and machine learning.<\/p>\n<p class=\"graf graf--p\">At Jumpshare<strong class=\"markup--strong markup--p-strong\">,<\/strong> where I hold the position of VP of Engineering, we have successfully turned these challenges into opportunities. We\u2019re passionate about implementing techniques like this to make our machine learning inference tasks more cost-effective. By leveraging the power of AWS Graviton2 and NVIDIA Tensor T4G instances, we\u2019ve been able to drastically reduce operational costs without compromising performance.<\/p>\n<p class=\"graf graf--p\">This guide is yet another effort to express our commitment to sharing our experience and insights with the community as we strongly believe that democratizing technology and saving costs on infrastructure can unlock doors to innovation.<\/p>\n<p class=\"graf graf--p\">We\u2019re always open to hearing about your own experiences and improvements on the journey towards cost-effective, high-performance deep learning.<\/p>\n<\/div>\n<\/div>\n<\/section>\n","protected":false},"excerpt":{"rendered":"<p>Note: The \u201cas low as free\u201d tagline is based on g5g.xlarge spot instance rates, which have been as low as $0.1519\/hr. Introduction The world we live in today heavily relies on artificial intelligence. From vacuum bots to sales support, from self-driving cars to disease detection, from finding the content you want to consume to translating [&hellip;]<\/p>\n","protected":false},"author":4,"featured_media":3396,"comment_status":"open","ping_status":"open","sticky":false,"template":"","format":"standard","meta":{"footnotes":""},"categories":[22],"tags":[],"class_list":["post-3392","post","type-post","status-publish","format-standard","has-post-thumbnail","hentry","category-engineering"],"yoast_head":"<!-- This site is optimized with the Yoast SEO plugin v19.8 - https:\/\/yoast.com\/wordpress\/plugins\/seo\/ -->\n<title>Deep Learning on AWS Graviton2, NVIDIA T4G for as Low as Free<\/title>\n<meta name=\"robots\" content=\"index, follow, max-snippet:-1, max-image-preview:large, max-video-preview:-1\" \/>\n<link rel=\"canonical\" href=\"https:\/\/jumpshare.com\/blog\/deep-learning-on-aws-graviton2-nvidia-tensor-t4g-for-as-low-as-free-with-cuda-12-2\/\" \/>\n<meta property=\"og:locale\" content=\"en_US\" \/>\n<meta property=\"og:type\" content=\"article\" \/>\n<meta property=\"og:title\" content=\"Deep Learning on AWS Graviton2, NVIDIA T4G for as Low as Free\" \/>\n<meta property=\"og:description\" content=\"Note: The \u201cas low as free\u201d tagline is based on g5g.xlarge spot instance rates, which have been as low as $0.1519\/hr. Introduction The world we live in today heavily relies on artificial intelligence. From vacuum bots to sales support, from self-driving cars to disease detection, from finding the content you want to consume to translating [&hellip;]\" \/>\n<meta property=\"og:url\" content=\"https:\/\/jumpshare.com\/blog\/deep-learning-on-aws-graviton2-nvidia-tensor-t4g-for-as-low-as-free-with-cuda-12-2\/\" \/>\n<meta property=\"og:site_name\" content=\"Jumpshare Blog\" \/>\n<meta property=\"article:publisher\" content=\"https:\/\/facebook.com\/jumpshare\" \/>\n<meta property=\"article:published_time\" content=\"2023-09-01T21:06:20+00:00\" \/>\n<meta property=\"og:image\" content=\"https:\/\/blog-cdn.jumpshare.com\/blog\/wp-content\/uploads\/2023\/09\/pexels-google-deepmind-18069365.jpg\" \/>\n\t<meta property=\"og:image:width\" content=\"1920\" \/>\n\t<meta property=\"og:image:height\" content=\"1080\" \/>\n\t<meta property=\"og:image:type\" content=\"image\/jpeg\" \/>\n<meta name=\"author\" content=\"Mirza Bilal\" \/>\n<meta name=\"twitter:card\" content=\"summary_large_image\" \/>\n<meta name=\"twitter:creator\" content=\"@jumpshare\" \/>\n<meta name=\"twitter:site\" content=\"@jumpshare\" \/>\n<meta name=\"twitter:label1\" content=\"Written by\" \/>\n\t<meta name=\"twitter:data1\" content=\"Mirza Bilal\" \/>\n\t<meta name=\"twitter:label2\" content=\"Est. reading time\" \/>\n\t<meta name=\"twitter:data2\" content=\"11 minutes\" \/>\n<script type=\"application\/ld+json\" class=\"yoast-schema-graph\">{\"@context\":\"https:\/\/schema.org\",\"@graph\":[{\"@type\":\"Article\",\"@id\":\"https:\/\/jumpshare.com\/blog\/deep-learning-on-aws-graviton2-nvidia-tensor-t4g-for-as-low-as-free-with-cuda-12-2\/#article\",\"isPartOf\":{\"@id\":\"https:\/\/jumpshare.com\/blog\/deep-learning-on-aws-graviton2-nvidia-tensor-t4g-for-as-low-as-free-with-cuda-12-2\/\"},\"author\":{\"name\":\"Mirza Bilal\",\"@id\":\"https:\/\/jumpshare.com\/blog\/#\/schema\/person\/d7e70e80b34722e97a0d7e1630db5efa\"},\"headline\":\"Deep Learning on AWS Graviton2, NVIDIA Tensor T4G for as Low as Free with CUDA-12.2\",\"datePublished\":\"2023-09-01T21:06:20+00:00\",\"dateModified\":\"2023-09-01T21:06:20+00:00\",\"mainEntityOfPage\":{\"@id\":\"https:\/\/jumpshare.com\/blog\/deep-learning-on-aws-graviton2-nvidia-tensor-t4g-for-as-low-as-free-with-cuda-12-2\/\"},\"wordCount\":1638,\"commentCount\":0,\"publisher\":{\"@id\":\"https:\/\/jumpshare.com\/blog\/#organization\"},\"articleSection\":[\"Engineering\"],\"inLanguage\":\"en-US\",\"potentialAction\":[{\"@type\":\"CommentAction\",\"name\":\"Comment\",\"target\":[\"https:\/\/jumpshare.com\/blog\/deep-learning-on-aws-graviton2-nvidia-tensor-t4g-for-as-low-as-free-with-cuda-12-2\/#respond\"]}]},{\"@type\":\"WebPage\",\"@id\":\"https:\/\/jumpshare.com\/blog\/deep-learning-on-aws-graviton2-nvidia-tensor-t4g-for-as-low-as-free-with-cuda-12-2\/\",\"url\":\"https:\/\/jumpshare.com\/blog\/deep-learning-on-aws-graviton2-nvidia-tensor-t4g-for-as-low-as-free-with-cuda-12-2\/\",\"name\":\"Deep Learning on AWS Graviton2, NVIDIA T4G for as Low as Free\",\"isPartOf\":{\"@id\":\"https:\/\/jumpshare.com\/blog\/#website\"},\"datePublished\":\"2023-09-01T21:06:20+00:00\",\"dateModified\":\"2023-09-01T21:06:20+00:00\",\"breadcrumb\":{\"@id\":\"https:\/\/jumpshare.com\/blog\/deep-learning-on-aws-graviton2-nvidia-tensor-t4g-for-as-low-as-free-with-cuda-12-2\/#breadcrumb\"},\"inLanguage\":\"en-US\",\"potentialAction\":[{\"@type\":\"ReadAction\",\"target\":[\"https:\/\/jumpshare.com\/blog\/deep-learning-on-aws-graviton2-nvidia-tensor-t4g-for-as-low-as-free-with-cuda-12-2\/\"]}]},{\"@type\":\"BreadcrumbList\",\"@id\":\"https:\/\/jumpshare.com\/blog\/deep-learning-on-aws-graviton2-nvidia-tensor-t4g-for-as-low-as-free-with-cuda-12-2\/#breadcrumb\",\"itemListElement\":[{\"@type\":\"ListItem\",\"position\":1,\"name\":\"Home\",\"item\":\"https:\/\/jumpshare.com\/blog\/\"},{\"@type\":\"ListItem\",\"position\":2,\"name\":\"Deep Learning on AWS Graviton2, NVIDIA Tensor T4G for as Low as Free with CUDA-12.2\"}]},{\"@type\":\"WebSite\",\"@id\":\"https:\/\/jumpshare.com\/blog\/#website\",\"url\":\"https:\/\/jumpshare.com\/blog\/\",\"name\":\"Jumpshare Blog\",\"description\":\"\",\"publisher\":{\"@id\":\"https:\/\/jumpshare.com\/blog\/#organization\"},\"potentialAction\":[{\"@type\":\"SearchAction\",\"target\":{\"@type\":\"EntryPoint\",\"urlTemplate\":\"https:\/\/jumpshare.com\/blog\/?s={search_term_string}\"},\"query-input\":\"required name=search_term_string\"}],\"inLanguage\":\"en-US\"},{\"@type\":\"Organization\",\"@id\":\"https:\/\/jumpshare.com\/blog\/#organization\",\"name\":\"Jumpshare\",\"url\":\"https:\/\/jumpshare.com\/blog\/\",\"sameAs\":[\"https:\/\/facebook.com\/jumpshare\",\"https:\/\/twitter.com\/jumpshare\"],\"logo\":{\"@type\":\"ImageObject\",\"inLanguage\":\"en-US\",\"@id\":\"https:\/\/jumpshare.com\/blog\/#\/schema\/logo\/image\/\",\"url\":\"https:\/\/blog-cdn.jumpshare.com\/blog\/wp-content\/uploads\/2022\/05\/Logo-white-new.png\",\"contentUrl\":\"https:\/\/blog-cdn.jumpshare.com\/blog\/wp-content\/uploads\/2022\/05\/Logo-white-new.png\",\"width\":2164,\"height\":572,\"caption\":\"Jumpshare\"},\"image\":{\"@id\":\"https:\/\/jumpshare.com\/blog\/#\/schema\/logo\/image\/\"}},{\"@type\":\"Person\",\"@id\":\"https:\/\/jumpshare.com\/blog\/#\/schema\/person\/d7e70e80b34722e97a0d7e1630db5efa\",\"name\":\"Mirza Bilal\",\"image\":{\"@type\":\"ImageObject\",\"inLanguage\":\"en-US\",\"@id\":\"https:\/\/jumpshare.com\/blog\/#\/schema\/person\/image\/\",\"url\":\"https:\/\/secure.gravatar.com\/avatar\/3c8a03ef46de571d70816e714d00b3ab?s=96&d=mm&r=g\",\"contentUrl\":\"https:\/\/secure.gravatar.com\/avatar\/3c8a03ef46de571d70816e714d00b3ab?s=96&d=mm&r=g\",\"caption\":\"Mirza Bilal\"},\"url\":\"https:\/\/jumpshare.com\/blog\/author\/mirza\/\"}]}<\/script>\n<!-- \/ Yoast SEO plugin. -->","yoast_head_json":{"title":"Deep Learning on AWS Graviton2, NVIDIA T4G for as Low as Free","robots":{"index":"index","follow":"follow","max-snippet":"max-snippet:-1","max-image-preview":"max-image-preview:large","max-video-preview":"max-video-preview:-1"},"canonical":"https:\/\/jumpshare.com\/blog\/deep-learning-on-aws-graviton2-nvidia-tensor-t4g-for-as-low-as-free-with-cuda-12-2\/","og_locale":"en_US","og_type":"article","og_title":"Deep Learning on AWS Graviton2, NVIDIA T4G for as Low as Free","og_description":"Note: The \u201cas low as free\u201d tagline is based on g5g.xlarge spot instance rates, which have been as low as $0.1519\/hr. Introduction The world we live in today heavily relies on artificial intelligence. From vacuum bots to sales support, from self-driving cars to disease detection, from finding the content you want to consume to translating [&hellip;]","og_url":"https:\/\/jumpshare.com\/blog\/deep-learning-on-aws-graviton2-nvidia-tensor-t4g-for-as-low-as-free-with-cuda-12-2\/","og_site_name":"Jumpshare Blog","article_publisher":"https:\/\/facebook.com\/jumpshare","article_published_time":"2023-09-01T21:06:20+00:00","og_image":[{"width":1920,"height":1080,"url":"https:\/\/blog-cdn.jumpshare.com\/blog\/wp-content\/uploads\/2023\/09\/pexels-google-deepmind-18069365.jpg","type":"image\/jpeg"}],"author":"Mirza Bilal","twitter_card":"summary_large_image","twitter_creator":"@jumpshare","twitter_site":"@jumpshare","twitter_misc":{"Written by":"Mirza Bilal","Est. reading time":"11 minutes"},"schema":{"@context":"https:\/\/schema.org","@graph":[{"@type":"Article","@id":"https:\/\/jumpshare.com\/blog\/deep-learning-on-aws-graviton2-nvidia-tensor-t4g-for-as-low-as-free-with-cuda-12-2\/#article","isPartOf":{"@id":"https:\/\/jumpshare.com\/blog\/deep-learning-on-aws-graviton2-nvidia-tensor-t4g-for-as-low-as-free-with-cuda-12-2\/"},"author":{"name":"Mirza Bilal","@id":"https:\/\/jumpshare.com\/blog\/#\/schema\/person\/d7e70e80b34722e97a0d7e1630db5efa"},"headline":"Deep Learning on AWS Graviton2, NVIDIA Tensor T4G for as Low as Free with CUDA-12.2","datePublished":"2023-09-01T21:06:20+00:00","dateModified":"2023-09-01T21:06:20+00:00","mainEntityOfPage":{"@id":"https:\/\/jumpshare.com\/blog\/deep-learning-on-aws-graviton2-nvidia-tensor-t4g-for-as-low-as-free-with-cuda-12-2\/"},"wordCount":1638,"commentCount":0,"publisher":{"@id":"https:\/\/jumpshare.com\/blog\/#organization"},"articleSection":["Engineering"],"inLanguage":"en-US","potentialAction":[{"@type":"CommentAction","name":"Comment","target":["https:\/\/jumpshare.com\/blog\/deep-learning-on-aws-graviton2-nvidia-tensor-t4g-for-as-low-as-free-with-cuda-12-2\/#respond"]}]},{"@type":"WebPage","@id":"https:\/\/jumpshare.com\/blog\/deep-learning-on-aws-graviton2-nvidia-tensor-t4g-for-as-low-as-free-with-cuda-12-2\/","url":"https:\/\/jumpshare.com\/blog\/deep-learning-on-aws-graviton2-nvidia-tensor-t4g-for-as-low-as-free-with-cuda-12-2\/","name":"Deep Learning on AWS Graviton2, NVIDIA T4G for as Low as Free","isPartOf":{"@id":"https:\/\/jumpshare.com\/blog\/#website"},"datePublished":"2023-09-01T21:06:20+00:00","dateModified":"2023-09-01T21:06:20+00:00","breadcrumb":{"@id":"https:\/\/jumpshare.com\/blog\/deep-learning-on-aws-graviton2-nvidia-tensor-t4g-for-as-low-as-free-with-cuda-12-2\/#breadcrumb"},"inLanguage":"en-US","potentialAction":[{"@type":"ReadAction","target":["https:\/\/jumpshare.com\/blog\/deep-learning-on-aws-graviton2-nvidia-tensor-t4g-for-as-low-as-free-with-cuda-12-2\/"]}]},{"@type":"BreadcrumbList","@id":"https:\/\/jumpshare.com\/blog\/deep-learning-on-aws-graviton2-nvidia-tensor-t4g-for-as-low-as-free-with-cuda-12-2\/#breadcrumb","itemListElement":[{"@type":"ListItem","position":1,"name":"Home","item":"https:\/\/jumpshare.com\/blog\/"},{"@type":"ListItem","position":2,"name":"Deep Learning on AWS Graviton2, NVIDIA Tensor T4G for as Low as Free with CUDA-12.2"}]},{"@type":"WebSite","@id":"https:\/\/jumpshare.com\/blog\/#website","url":"https:\/\/jumpshare.com\/blog\/","name":"Jumpshare Blog","description":"","publisher":{"@id":"https:\/\/jumpshare.com\/blog\/#organization"},"potentialAction":[{"@type":"SearchAction","target":{"@type":"EntryPoint","urlTemplate":"https:\/\/jumpshare.com\/blog\/?s={search_term_string}"},"query-input":"required name=search_term_string"}],"inLanguage":"en-US"},{"@type":"Organization","@id":"https:\/\/jumpshare.com\/blog\/#organization","name":"Jumpshare","url":"https:\/\/jumpshare.com\/blog\/","sameAs":["https:\/\/facebook.com\/jumpshare","https:\/\/twitter.com\/jumpshare"],"logo":{"@type":"ImageObject","inLanguage":"en-US","@id":"https:\/\/jumpshare.com\/blog\/#\/schema\/logo\/image\/","url":"https:\/\/blog-cdn.jumpshare.com\/blog\/wp-content\/uploads\/2022\/05\/Logo-white-new.png","contentUrl":"https:\/\/blog-cdn.jumpshare.com\/blog\/wp-content\/uploads\/2022\/05\/Logo-white-new.png","width":2164,"height":572,"caption":"Jumpshare"},"image":{"@id":"https:\/\/jumpshare.com\/blog\/#\/schema\/logo\/image\/"}},{"@type":"Person","@id":"https:\/\/jumpshare.com\/blog\/#\/schema\/person\/d7e70e80b34722e97a0d7e1630db5efa","name":"Mirza Bilal","image":{"@type":"ImageObject","inLanguage":"en-US","@id":"https:\/\/jumpshare.com\/blog\/#\/schema\/person\/image\/","url":"https:\/\/secure.gravatar.com\/avatar\/3c8a03ef46de571d70816e714d00b3ab?s=96&d=mm&r=g","contentUrl":"https:\/\/secure.gravatar.com\/avatar\/3c8a03ef46de571d70816e714d00b3ab?s=96&d=mm&r=g","caption":"Mirza Bilal"},"url":"https:\/\/jumpshare.com\/blog\/author\/mirza\/"}]}},"jetpack_featured_media_url":"https:\/\/blog-cdn.jumpshare.com\/blog\/wp-content\/uploads\/2023\/09\/pexels-google-deepmind-18069365.jpg","_links":{"self":[{"href":"https:\/\/jumpshare.com\/blog\/wp-json\/wp\/v2\/posts\/3392"}],"collection":[{"href":"https:\/\/jumpshare.com\/blog\/wp-json\/wp\/v2\/posts"}],"about":[{"href":"https:\/\/jumpshare.com\/blog\/wp-json\/wp\/v2\/types\/post"}],"author":[{"embeddable":true,"href":"https:\/\/jumpshare.com\/blog\/wp-json\/wp\/v2\/users\/4"}],"replies":[{"embeddable":true,"href":"https:\/\/jumpshare.com\/blog\/wp-json\/wp\/v2\/comments?post=3392"}],"version-history":[{"count":28,"href":"https:\/\/jumpshare.com\/blog\/wp-json\/wp\/v2\/posts\/3392\/revisions"}],"predecessor-version":[{"id":3399,"href":"https:\/\/jumpshare.com\/blog\/wp-json\/wp\/v2\/posts\/3392\/revisions\/3399"}],"wp:featuredmedia":[{"embeddable":true,"href":"https:\/\/jumpshare.com\/blog\/wp-json\/wp\/v2\/media\/3396"}],"wp:attachment":[{"href":"https:\/\/jumpshare.com\/blog\/wp-json\/wp\/v2\/media?parent=3392"}],"wp:term":[{"taxonomy":"category","embeddable":true,"href":"https:\/\/jumpshare.com\/blog\/wp-json\/wp\/v2\/categories?post=3392"},{"taxonomy":"post_tag","embeddable":true,"href":"https:\/\/jumpshare.com\/blog\/wp-json\/wp\/v2\/tags?post=3392"}],"curies":[{"name":"wp","href":"https:\/\/api.w.org\/{rel}","templated":true}]}}